{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c255f2a-e669-49b9-b7e6-aa71f504d5bc",
   "metadata": {},
   "source": [
    "# Clustering and feature extraction pipeline\n",
    "### input : all the csv files processed with Preprocessing_voltage.ipynb\n",
    "### output : extracted features from cluster, ROI wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89df17a-905c-4e6a-b0db-44efdd88d21d",
   "metadata": {},
   "source": [
    "### import data, append it to make single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage \n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "all_files = os.listdir(\"./data\")\n",
    "csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "print(csv_files)\n",
    "\n",
    "global_min = -0.04\n",
    "global_max = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32905dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframes_list = []\n",
    "\n",
    "sample_dict = {}\n",
    "\n",
    "for file_path in csv_files:\n",
    "    df = pd.read_csv(\"./data/\"+file_path)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    \n",
    "    # Extract sample name\n",
    "    sample_name = '_'.join(file_path.split('_')[:3])\n",
    "    \n",
    "    if sample_name not in sample_dict:\n",
    "        sample_dict[sample_name] = []\n",
    "    \n",
    "    sample_dict[sample_name].append(df)\n",
    "\n",
    "# Normalize data within each sample group\n",
    "normalized_dataframes_list = []\n",
    "\n",
    "for sample_name, dfs in sample_dict.items():\n",
    "    sample_df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    # Perform normalization for the sample\n",
    "    df_max = min(sample_df.values.max(), 0.1)\n",
    "    sample_df = sample_df * (0.1 / sample_df.values.max())\n",
    "    \n",
    "    # Convert numpy array back to DataFrame to maintain column headers\n",
    "    sample_df = pd.DataFrame(sample_df, columns=sample_df.columns)\n",
    "    \n",
    "    normalized_dataframes_list.append(sample_df)\n",
    "\n",
    "# Concatenate all normalized dataframes\n",
    "data = pd.concat(normalized_dataframes_list, axis=1)\n",
    "\n",
    "# Optionally drop specific columns if needed\n",
    "segment_error = []\n",
    "data.drop(columns = segment_error, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df3c4d-e1f6-44b7-b61a-49afd3e7257a",
   "metadata": {},
   "source": [
    "### extract features (peak_height, slope(varable name : response_dur for legacy), peak_b(binary) to make a dataframe\n",
    "\n",
    "#### these features are obtained each stimulation : total of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca47c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#prarameter for clustering\n",
    "\n",
    "param_margin = 2 #how much margin would you allow up to before stim\n",
    "param_div = 5 #how much frames after stim that you would define as stim response\n",
    "\n",
    "# thresholds for binary peak sorting\n",
    "threshold_peak_value = 0.035 \n",
    "threshold_PSNR = 8\n",
    "\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "\n",
    "stimulus_frames = [50 * x for x in range(1,11)]\n",
    "\n",
    "# Create new DataFrame to store features\n",
    "\n",
    "peak_response_df = pd.DataFrame()\n",
    "idx = 0\n",
    "\n",
    "for column in data.columns:\n",
    "    peak_responses = []\n",
    "    for frame in stimulus_frames:\n",
    "        window_start = frame - param_margin\n",
    "        \n",
    "        window_end = frame + 50 - param_margin\n",
    "        \n",
    "        window = data.loc[window_start:window_end, column]\n",
    "        \n",
    "        peak_value = window[0 : param_margin + param_div].max()\n",
    "        \n",
    "        peak_auc = window[0 : param_margin + param_div].sum()\n",
    "        \n",
    "        response_dur = peak_value - part2\n",
    "\n",
    "        auc = window[0:10].sum()\n",
    "        \n",
    "        window_mean = window.mean()\n",
    "        \n",
    "\n",
    "        \n",
    "        MSE_ref = np.mean((part4 - part4.mean())**2)\n",
    "        PSNR = 20 * np.log10(peak_value / np.sqrt(MSE_ref))\n",
    "        \n",
    "        if((peak_value > threshold_peak_value) & (PSNR > threshold_PSNR)):\n",
    "            peak_b = 1\n",
    "        else:\n",
    "            peak_b = 0\n",
    " \n",
    "        peak_responses.extend([peak_value, response_dur, peak_b])\n",
    "\n",
    "\n",
    "    peak_response_df[column] = peak_responses\n",
    "    idx += 1\n",
    "\n",
    "extracted = peak_response_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c43f3-e183-4f78-9eb7-37205f7d169d",
   "metadata": {},
   "source": [
    "#### features extracted from each peaks are averaged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ff284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of each feature\n",
    "\n",
    "extracted['11'] = extracted.iloc[:,np.arange(0,30,3)].mean(axis = 1)\n",
    "extracted['21'] = extracted.iloc[:,np.arange(1,30,3)].mean(axis = 1)\n",
    "extracted['32'] = extracted.iloc[:,np.arange(2,30,3)].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53dfee-d192-437a-82b4-47ce8b63213f",
   "metadata": {},
   "source": [
    "### PCA plotting to see overall feature map (actuall clustering is done in HCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2485e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(extracted[['11', '21', '32']])\n",
    "\n",
    "\n",
    "#visualization\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame of the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plotting the PCA results\n",
    "# Initialize and fit PCA, here we make sure to keep only the first two components for vis\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# display explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained variance by component:\", explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b94c1-db12-400a-954a-7f6cd1530aaa",
   "metadata": {},
   "source": [
    "### HCA clustering using the scale data from above shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae96fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "k = 3\n",
    "\n",
    "data_t = data.T\n",
    "\n",
    "# initialization of AggolomertiveClustering and fit\n",
    "hca = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "labels = hca.fit_predict(scaled_data)                              \n",
    "\n",
    "# add resulting labels to orginal dataframe\n",
    "data_t['cluster'] = labels\n",
    "\n",
    "# output results\n",
    "print(data_t.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0d137-942f-4047-ae5b-b75461c6dd27",
   "metadata": {},
   "source": [
    "### plotting Averaged traces from each cluster (for examination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c801e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = data_t.shape[1]-1  # Number of time points in your DataFrame\n",
    "time_vector = np.arange(0, num_points * 0.01, 0.01)  # Create a time vector that matches your data\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(k, 1, figsize=(10, k * 3), sharex=True)\n",
    "\n",
    "global_min = -0.04\n",
    "global_max = 0.12\n",
    "\n",
    "for i in range(k):\n",
    "    cluster_data = data_t[data_t['cluster'] == i].drop('cluster', axis=1)\n",
    "    print(f'Cluster {i}')\n",
    "    print(cluster_data.T.columns)\n",
    "    filename = f'clusteredoutput/original_cluster_{i}.csv'\n",
    "    cluster_data.T.to_csv(filename)\n",
    "    \n",
    "    for index, row in cluster_data.iterrows():\n",
    "        axs[i].plot(time_vector, row, alpha=0.3, color = 'gray')\n",
    "    \n",
    "    axs[i].plot(time_vector, cluster_data.mean(axis=0), alpha = 1, color = 'r')\n",
    "    \n",
    "    axs[i].set_title(f'Cluster {i}')\n",
    "    axs[i].set_xlabel('Time (s)')\n",
    "    axs[i].set_ylabel('dF/F0')\n",
    "    axs[i].set_ylim(global_min, global_max)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f783e4-afc2-4098-9987-f542a6653de5",
   "metadata": {},
   "source": [
    "### additional function for mapping specific dataset to integer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetmap(date_str):\n",
    "    \n",
    "    date_map = {\n",
    "        '240122_Grid2': 1,\n",
    "        '240418_Grid1': 2,\n",
    "        '240529_Grid1': 3,\n",
    "        '250329_Grid2': 4\n",
    "    }\n",
    "    \n",
    "    return date_map.get(date_str,0)\n",
    "\n",
    "def dtostr(date, grid, roi, num):\n",
    "    return str(date) + '_Grid' + str(grid) + '_ROI' + str(roi) + '_#' + str(num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bb119-664d-48d6-a332-0a6d9d8d854e",
   "metadata": {},
   "source": [
    "### PCA plot for examination (including identification whether we got EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# include the label if we got EM or not\n",
    "em_list = pd.read_csv('metadata/metadata.csv')\n",
    "em_list = em_list['Voltage'].tolist()\n",
    "\n",
    "count = 0\n",
    "\n",
    "data_t['em']=0\n",
    "for row in data_t.iterrows():\n",
    "    if(row[0] in em_list):\n",
    "        data_t.loc[row[0],'em']= 1\n",
    "        count += 1\n",
    "        \n",
    "data_t['sample_name'] = data_t.index.str.split('_').str[0] + '_' + data_t.index.str.split('_').str[1]\n",
    "data_t['sample_name'] = data_t['sample_name'].apply(datasetmap)\n",
    "\n",
    "data_t.to_csv(\"metadata/clusterandem.csv\")\n",
    "        \n",
    "        \n",
    "# PCA plotting\n",
    "X = scaled_data\n",
    "y = data_t['cluster'].values  # Labels\n",
    "z = data_t['em'].values\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['r', 'g', 'b','k', 'c', 'y']\n",
    "for i, color in zip(range(k), colors):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=color, label=f'Cluster {i}')\n",
    "\n",
    "    \n",
    "alphas = [0,0.5,0]\n",
    "for i, color, alpha in zip(range(k), colors, alphas):\n",
    "    plt.scatter(X_pca[z == i, 0], X_pca[z == i, 1], c='w', alpha = alphas[i], edgecolor='gold', linewidth=1.5, s=100)\n",
    "\n",
    "for i, (x, y, name) in enumerate(zip(X_pca[:, 0], X_pca[:, 1], sample_name)):\n",
    "    if data_t.iloc[i]['em'] == 1:\n",
    "        plt.scatter(x, y, c='w', alpha = 0.5, edgecolor='gold', linewidth=1.5, s=100)\n",
    "        plt.text(x, y, name, fontsize=9, ha='right')  \n",
    "\n",
    "plt.title('PCA visualization of the time series clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.savefig(\"fig.eps\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798346ea-6cfc-492a-b999-94b833ac576e",
   "metadata": {},
   "source": [
    "### Generate clustering heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872c6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [\n",
    "    (0.0, '#000000'),   # Black at 0.0\n",
    "    (0.1, '#000000'),  # Black at 0.04\n",
    "    (0.3, '#8800FF'), # Dark purple at 0.045\n",
    "    (0.5, '#FF0000'),  # Red at 0.05\n",
    "    (0.7, '#FFFF00'), # Yellow at 0.055\n",
    "    (1, \"#FFFF00\")\n",
    "]\n",
    "\n",
    "\n",
    "# Create the colormap\n",
    "cmap = LinearSegmentedColormap.from_list('my_cmap', colors)\n",
    "\n",
    "save = False\n",
    "\n",
    "\n",
    "# Generate the linkage matrix using the Ward method\n",
    "Z = linkage(scaled_data, method='ward')\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(15, 50))\n",
    "\n",
    "# Add dendrogram axis\n",
    "ax_dendro = fig.add_axes([0.09, 0.1, 0.2, 0.6])  # x, y, width, height\n",
    "dn = dendrogram(Z, orientation='left', ax=ax_dendro, no_plot = True)\n",
    "ax_dendro.set_xticks([])\n",
    "ax_dendro.set_yticks([])\n",
    "\n",
    "dn['leaves'] = dn['leaves'][::-1]\n",
    "dendrogram(Z, orientation='left', ax=ax_dendro, no_labels=True, leaf_rotation=0, leaf_font_size=12)\n",
    "\n",
    "# Add heatmap axis\n",
    "kwargs = {'rasterized' : True}\n",
    "ax_heatmap = fig.add_axes([0.3, 0.1, 0.6, 0.6])\n",
    "data_reordered = ((data_t.drop('cluster', axis =1)).drop('em', axis = 1)).drop('sample_name', axis = 1).iloc[dn['leaves']]  # Reorder data based on the dendrogram\n",
    "sns.heatmap(data_reordered, cmap=cmap, ax=ax_heatmap, vmin=0.00, vmax=0.1, **kwargs, \n",
    "            cbar_kws={'label': 'Scale', 'shrink' : 0.2, 'location':'right'})\n",
    "\n",
    "ax_cluster = fig.add_axes([0.9, 0.1, 0.1,0.6])\n",
    "data_reordered = data_t.iloc[dn['leaves']]\n",
    "sns.heatmap(data_reordered[['cluster','em','sample_name']], cmap='rainbow', ax=ax_cluster, vmin=0.0, vmax=6,  **kwargs, \n",
    "            cbar_kws={'label': 'cluster, em', 'shrink' : 0.5, 'location' : 'right'})\n",
    "\n",
    "\n",
    "# Adjusting x-axis to show time in seconds\n",
    "frame_to_time = lambda x: x * 10 / 1000  # Convert frames to seconds (10 ms per frame)\n",
    "num_frames = data_reordered.shape[1]\n",
    "time_ticks = [frame_to_time(x) for x in range(0, num_frames, int(num_frames / 10))]  # Generate time ticks\n",
    "ax_heatmap.set_xticks(np.linspace(0, num_frames, len(time_ticks)))\n",
    "ax_heatmap.set_xticklabels(['{:.2f} s'.format(t) for t in time_ticks])\n",
    "ax_heatmap.set_xlabel('Time (s)')\n",
    "ax_heatmap.set_ylabel('Feature Index')\n",
    "ax_heatmap.set_yticks(data_t['cluster']);\n",
    "ax_cluster.set_yticks([])\n",
    "\n",
    "plt.savefig(\"fig.eps\", format = 'eps', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40983309",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data\n",
    "y = data_t['sample_name'].values  # Labels\n",
    "z = data_t['em'].values\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['k', 'k', 'k','k', 'k', 'k','k','k','r']\n",
    "#colors = ['r', 'g', 'b','k', 'c']\n",
    "for i, color in zip(range(9), colors):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=color, label=f'dataset {i}')\n",
    "    \n",
    "    \n",
    "\n",
    "plt.title('PCA visualization of the time series clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.savefig(\"asd.eps\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d17f2-7dbc-4034-bd7d-996dbc9c6364",
   "metadata": {},
   "source": [
    "### Finally, save statistics of each ROIs this includes EM, cluster and integer sample name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = extracted\n",
    "\n",
    "statistics['clusters'] = labels\n",
    "statistics['em'] = data_t['em']\n",
    "statistics['sample_name'] = data_t['sample_name']\n",
    "\n",
    "statistics.to_csv(\"metadata/stat_250628.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
